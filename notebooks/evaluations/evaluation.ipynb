{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook has examples on how to simulate chats and how to run bulk evaluation.\n",
    "\n",
    " ❗ Before running this notebook, make sure you install the dependencies defined in `src/requirements-eval.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "The evaluation module depends on other modules from project Acumen, specifically for loading conversation histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SOURCE_DIR = \"../../src\"\n",
    "sys.path.append(SOURCE_DIR)\n",
    "\n",
    "from group_chat import create_group_chat\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(SOURCE_DIR, \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import load_agent_config, setup_logging\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "agent_config = load_agent_config(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob.aio import BlobServiceClient\n",
    "from data_models.data_access import DataAccess\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "blob_service_client = BlobServiceClient(\n",
    "    account_url=os.getenv(\"APP_BLOB_STORAGE_ENDPOINT\"),\n",
    "    credential=credential,\n",
    ")\n",
    "data_access = DataAccess(blob_service_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_QUERIES_CSV_PATH = \"./evaluation_sample_initial_queries.csv\"\n",
    "SIMULATION_OUTPUT_PATH = \"../data/simulated_chats/patient_4\"\n",
    "EVALUATION_RESULTS_PATH = os.path.join(SIMULATION_OUTPUT_PATH, \"evaluation_results\")\n",
    "\n",
    "PATIENT_TIMELINE_REFERENCE_PATH = \"../data/patient_timeline_reference/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate chats\n",
    "\n",
    "Below, we simulate conversations based on queries loaded from a `csv`. This csv must have one column with the **patient ID** (as expected by the agents) and an **initial query column**, that will serve as the conversation starter. Optionally, we may include an additional column for **follow-up questions**.\n",
    "\n",
    "Each row in the `csv` contains a single follow-up question. When `group_followups=True` (default), the system will combine all follow-ups with the same patient ID and initial query into a single conversation flow, asking them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.chat_simulator import ProceedUser, LLMUser, ChatSimulator\n",
    "\n",
    "initial_query = \"Orchestrator: Prepare tumor board for Patient ID: patient_4\"\n",
    "\n",
    "# user = ProceedUser()\n",
    "user = LLMUser()\n",
    "\n",
    "chat_simulator = ChatSimulator(\n",
    "    simulated_user=user,\n",
    "    group_chat_kwargs={\n",
    "        \"all_agents_config\": agent_config,\n",
    "        \"data_access\": data_access,\n",
    "    },\n",
    "    trial_count=1,\n",
    "    max_turns=10,\n",
    "    output_folder_path=SIMULATION_OUTPUT_PATH,\n",
    "    save_readable_history=True,\n",
    "    print_messages=False,\n",
    "    raise_errors=True,\n",
    ")\n",
    "\n",
    "chat_simulator.load_initial_queries(\n",
    "    csv_file_path=INITIAL_QUERIES_CSV_PATH,\n",
    "    patients_id_column=\"Patient ID\",\n",
    "    initial_queries_column=\"Initial Query\",\n",
    "    followup_column=\"Possible Follow up\",\n",
    "    group_followups=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling `load_initial_queries` to load the data needed for simulating chats, you may pass them directly to the class constructor:\n",
    "\n",
    "```python\n",
    "patient_id = \"patient_4\"\n",
    "initial_query = \"Orchestrator: Prepare tumor board for Patient ID: patient_4\"\n",
    "# At least an empty string must be given as a followup question\n",
    "followup_questions = [\"\"]\n",
    "\n",
    "user = LLMUser()\n",
    "\n",
    "chat_simulator = ChatSimulator(\n",
    "    simulated_user=user,\n",
    "    group_chat_kwargs={\n",
    "        \"all_agents_config\": agent_config,\n",
    "        \"data_access\": data_access,\n",
    "    },\n",
    "    patients_id=[patient_id],\n",
    "    initial_queries=[initial_query],\n",
    "    followup_questions=[followup_questions],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of output is generated when simulating chats, which might make this file too big for opening.\n",
    "\n",
    "For that reason, please clear the output of at least the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat_simulator.simulate_chats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ad-hoc cases, you may also call `chat_simulator.chat` directly:\n",
    "\n",
    "```python\n",
    "chat_simulator.chat(\n",
    "    patients_id=[patient_id],\n",
    "    initial_queries=[initial_query],\n",
    "    followup_questions=[followup_questions],\n",
    "    max_turns=5\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Input data\n",
    "Below, we evaluate conversations simulated in the previous step. For evaluation, we need the serialized chat context, which is out case it the `json` file generated by the simulation.\n",
    "\n",
    "The deployed application also stores conversations whenever they are cleared with the message `@Orchestrator clear`. Naturally, that data can also be used for evaluation.\n",
    "\n",
    "### Reference based metrics\n",
    "Below you will also notice that some metric (such as `RougeMetric` and `TBFactMetric`) require ground truth data to generate scores. Internally, these metrics, will load `.txt` files from a provided folder. **The `txt` file name must be the respective `patient_id`**\n",
    "\n",
    "> 💡**Tip**: The chat context `json` includes a top-level key `patient_id` that is used to track what patient was the target of the conversation, and thus used to match the correct reference data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "\n",
    "from evaluation.evaluator import Evaluator\n",
    "from evaluation.metrics.agent_selection import AgentSelectionEvaluator\n",
    "from evaluation.metrics.context_relevancy import ContextRelevancyEvaluator\n",
    "from evaluation.metrics.info_aggregation import InformationAggregationEvaluator\n",
    "from evaluation.metrics.rouge import RougeMetric\n",
    "from evaluation.metrics.intent_resolution import IntentResolutionEvaluator\n",
    "from evaluation.metrics.factuality import TBFactMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_service = AzureChatCompletion(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "agent_selection_evaluator = AgentSelectionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "intent_resolution_evaluator = IntentResolutionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "information_aggregation_evaluator = InformationAggregationEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "context_relevancy_evaluator = ContextRelevancyEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    agent_name=\"PatientHistory\",\n",
    "    context_window=5,\n",
    ")\n",
    "\n",
    "rouge_metric = RougeMetric(\n",
    "    agent_name=\"PatientHistory\",\n",
    "    reference_dir_path=PATIENT_TIMELINE_REFERENCE_PATH\n",
    ")\n",
    "\n",
    "tbfact_metric = TBFactMetric(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    agent_name=\"PatientHistory\",\n",
    "    reference_dir_path=PATIENT_TIMELINE_REFERENCE_PATH,\n",
    "    context_window=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    metrics=[\n",
    "        agent_selection_evaluator,\n",
    "        # intent_resolution_evaluator,\n",
    "        # information_aggregation_evaluator,\n",
    "        # context_relevancy_evaluator,\n",
    "        # rouge_metric,\n",
    "        tbfact_metric,\n",
    "    ],\n",
    "    output_folder_path=EVALUATION_RESULTS_PATH,\n",
    ")\n",
    "\n",
    "evaluator.load_chat_contexts(SIMULATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `ChatSimulator` class, you may skip `load_chat_contexts` by passing it directly in the constructor:\n",
    "\n",
    "```python\n",
    "from data_models.chat_context import ChatContext\n",
    "\n",
    "chats_contexts: list[ChatContext]\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    chats_contexts=chats_contexts\n",
    "    metrics=[\n",
    "        ...\n",
    "    ],\n",
    "    output_folder_path=SIMULATION_OUTPUT_PATH,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = await evaluator.evaluate()\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick show of results, we print results below, but for better understanding the scores and behaviour of agents, ideally you drill down the dictionary generated in the previous step (also saved as a `json` in the evaluation output folder). It includes all individual results, explanations and details specific of each metric.\n",
    "\n",
    ">💡**Note:** When no reference data is provided, that instance will result in `Error: No reference found for patient ID: `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric_result in evaluation_results[\"metrics\"].items():\n",
    "    print(f\"{metric_name}: average_score: {metric_result[\"average_score\"]} | num_errors: {metric_result[\"num_errors\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
